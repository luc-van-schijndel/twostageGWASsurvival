---
title: "twostageGWASsurvival"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{twostageGWASsurvival}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

The package implements a two stage method for finding significant interaction effects within a high-dimensional setting where the covariates outnumber the subjects assuming a Cox proportional hazards model.

Since the package is not (yet) available on CRAN, installation goes as follows:

```{r setup}
devtools::install_github("luc-van-schijndel/twostageGWASsurvival", quiet = TRUE)
library(twostageGWASsurvival)
```

The two stage method is, as the name implies, based on two stages. The first stage tests all covariates individually for a significant marginal effect. The covariates that are found to be marginally significant are passed on to the next stage. The second stage then tests all pairs of marginal significant for a significant interaction effect. Since we are testing multiple hypotheses, we have to correct for this fact, which is of course done by applying a multiple hypotheses correction such as the Bonferroni correction for example. The crucial part is that the two stages, more specifically the p-values found in the two stages, are independent of each other. The proof of this is quite long, but it follows from a thorough analysis of the Fisher information matrix. By this independence, we only need to apply the multiple hypotheses correction to the hypotheses tested in the second stage.

By viewing the first stage as a filter of some sort, you get to see the power of this two stage method. Due to the filter, the number of hypotheses tested in the second stage decreases quickly as the filter becomes more strict. The number of pairs tested in the second stage grows quadratically, so if only one in one hundred covariates are found to be marginally significant, the second stage tests only one in every ten thousand pairs.

This allows for a fast computation to a relatively complex problem. Especially in genetic research, this is a big boon, since the number of genetic covariates can be as large as several million and consequently, the number of pairs of covariates can be as large as several trillion. By applying a strict filter, this number of pairs can be reduced by a large enough factor that the analysis of these interactions becomes possible.

# Usage

This filter is specified by the value of the first stage threshold. This is the threshold the p-values of the first stage need to beat in order to be passed to the second stage. Under the null hypotheses, these p-values are uniformly distributed, so we expect that the fraction specified by the first stage threshold would be marginally significant and therefore passed to the second stage. This value can be tweaked by the user to achieve a suitable analysis.

By lowering the first stage threshold, less pairs of covariates will be tested in the second stage, resulting in a less strict multiple hypotheses correction and therefore increasing power. This is another benefit, aside from decreasing computation times. The filter must not be too strict however, since we do want any actual effects to be detected. Consider this value therefore properly.

# Example uses

A small example is given by the following datasets:

```{r small example}
survival.dataset <- survival::Surv(c(5,5,3,3,2,2,2,1,1,1),
                                   c(0,0,1,1,1,1,1,1,1,1))
covariate.matrix <- matrix(c(2,2,1,
                             2,2,1,
                             1,2,1,
                             2,1,1,
                             1,1,1,
                             1,1,1,
                             1,0,0,
                             0,1,0,
                             1,0,0,
                             0,0,0),
                           nrow = 10, ncol = 3, byrow = TRUE)
```

Looking closely at this data, we find that all covariates have a positive influence on the time to event since we find the highest values of the covariates for the highest survival times. The main function of this package, \code{twostagecoxph} can be used to analyse this tiny dataset assuming a Cox proportional hazards model:

```{r analysing brief example dataset}
result <- twostagecoxph(survival.dataset, covariate.matrix,
                        control = twostagecoxph.control(progress = 0))
print(result)
```

The object returned from the function has a print method associated with it, and this method provides a quick summary of the results. The object itself contains all aspects of the result, such as the vector of p-values found in the first stage, and the resulting p-values from the second stage arranged in a sparse matrix.

The function also has parallel processing capabilities, but these require a parallel back-end to be registered beforehand. Since the previous example is too small to profit from the multicore capabilities, let us introduce a toy GWAS dataset that comes with the package:

```{r larger toy GWAS dataset}
str(example_survival_data)
str(example_snp_data)
```

In this synthetic GWAS, we had 200 subjects of which we documented 500 SNPs. These SNPs have a strong correlation structure, as we often see in covariates derived from the (human) genome. One pair has a build in influence on the hazard rate according to a Cox proportional hazards model. Lets register a parallel back-end and use the package to analyse this dataset:

```{r analysing toy GWAS in parallel}
doParallel::registerDoParallel(2)
result <- twostagecoxph(example_survival_data, example_snp_data, multicore = TRUE,
                        control = twostagecoxph.control(progress = 0))
print(result)
```

The functions finds some significant results one of which was the interaction used to generate the data. Three pairs of covariates have a corresponding p-value that is lower than 0.05, so at this level of significance we find a significant interaction. Moreover, some other pairs have the same (significant) p-values as other pairs. All of these are listed in the \code{result} object:

```{r some results may be similar}
result$most.significant.results$duplicate.interactions
```

This means that the interaction of these pairs are so similar to the interactions displayed in \code{result}, that the p-values differ too little. These are therefore reported in a separate part of the object.

# Memory

This function is written with the intention that it would be used in a Genome Wide Association Study. In such a study, the amount of covariates can rise to the millions, which gives rise to problems concerning memory. To address this, a function has been implemented to read to covariates in batches from multiple files. This way, the user has control over how much memory is required for the operation of the function. The user has to provide the files themselves, and a way for the function to read them. The large variety of data structures does not allow for an easy way for the function the adapt the files to something useable, so the user is therefore required to do some work beforehand. The files should be structured in such a way, that the user-provided function results in a matrix where each column corresponds to a covariate. Additional requirements are described in the documentation. Lets make some files from the previous example to show how to pass files to the package:

```{r splitting toy GWAS into multiple files}
number.of.covs <- dim(example_snp_data)[2]
# Split the covariate matrix into various files.
number.of.files <- 6 
# 500 is not a multiple of 6, so the last file has less covariates than the other ones

temp.snpfile.paths <- tempfile(rep("snpfile", number.of.files),
                               tmpdir = tempdir(check = TRUE),
                               fileext = ".txt")
indices.matrix <- matrix(c(seq_len(number.of.covs),
                           rep(NA, ceiling(number.of.covs/number.of.files)*
                                 number.of.files - number.of.covs)),
                         ncol = number.of.files)
for(file.num in 1:number.of.files){
  indices <- indices.matrix[,file.num]
  write.table(example_snp_data[,indices[!is.na(indices)]], 
              file = temp.snpfile.paths[file.num])
}

```

The batch function does allows for the last file to have less covariates than the other files. This is however the only flexibility implemented; the function assumes all other files are structured similarly, e.g. same dimensions, same presence of headers, etc.

The result from the function using batches is then the same as the one using the basic function:

```{r analyse multiple files}
print(batched.result <- 
        batched.twostagecoxph(example_survival_data, temp.snpfile.paths,
                              number.of.covariates = number.of.covs,
                              read.function = function(x) as.matrix(read.table(x))
                              )
      )
```

It is important to pass the necessary function to read the files into a matrix. The files can be in any format, so long as the `read.function` parses the files into the correct matrix, with the appropriate aforementioned constraints. By default, the function `function(x) as.matrix(read.table(x))` is used, so in this explicit example this parameter was unnecessary, but for clarity of how to use the function was specified explicitly. 
Note that we must provide the paths to the files as a vector, and that we let the function know how many covariates can be found in all files combined. The function does allow that to be done automatically, but this requires it to read through all files individually, which costs time. The user probably already knows this, so it is best if it is provided. Also note that this function performs its operations in parallel, just like the previous example with \code{multicore = TRUE}.

When using this function, a consideration must be made concerning the amount of covariates in each file. The function always has no more than the contents of two files in memory on each core, but, it also needs some additional memory for fitting the model, and saving the results, etc. It is therefore recommended that no more than one quarter of the available memory of each core would be used by one file, just to be on the safe side. Contrary to that, it is also recommended that the files are as large as possible as every file has some overhead, e.g. creating the connection. Making the files as large as possible minimizes the number of files, while not impacting the overall analysis. 

# Additional possibilities

This package may be too limited in its implementation for all users. Since all research is different, we have decided to not try to implement too much variation, but only provide the most basic functioning package. If a user needs additional functionality, they are highly encouraged to implement their own adaptations. In order to aid this, a highly documented version of the source code is available on the GitHub repository of this package. This should help you in avoiding the many pitfalls lurking in implementing this two stage method.
