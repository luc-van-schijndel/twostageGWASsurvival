% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/batched_twostagecoxph.R
\name{batched.secondstagecoxph}
\alias{batched.secondstagecoxph}
\title{Multicore method of performing the second stage}
\usage{
batched.secondstagecoxph(
  survival.dataset,
  covariate.filepaths,
  first.stage.threshold,
  progress = 50,
  max.coef = 5,
  updatefile = "",
  max.batchsize = 1000,
  upper.bound.correlation = 0.95,
  read.function = function(x) as.matrix(read.table(x)),
  number.of.covariates,
  snps.are.named = FALSE,
  number.of.subjects
)
}
\arguments{
\item{survival.dataset}{the outcome data}

\item{covariate.filepaths}{paths to the files containing the covariates}

\item{first.stage.threshold}{the FST}

\item{progress}{set to 0 for no updates}

\item{max.coef}{maximum value of fitted weights before declared non-converged}

\item{updatefile}{path to that file}

\item{max.batchsize}{max number of covariates in one batch}

\item{upper.bound.correlation}{upper bound on the correlation before not checked}

\item{read.function}{the function used to read covariate batches}

\item{number.of.covariates}{combined total of covariates in all files}

\item{snps.are.named}{boolean, whether or not the snps will be named when read from file.}

\item{number.of.subjects}{Total number of subjects in all files}
}
\value{
list of p-value matrix, first stage p-values and which ones passed.
}
\description{
Multicore method of performing the second stage
}
\details{
Similarly to the multicore method of the first stage, this function works with
batches of covariates to alleviate possible memory issues. The optimal size of the batches
is calculated in a similar fashion as during the first stage, only here we halve the maximum
batchsize, since (almost always) two batches of covariates will be in memory at the same time.

The testing for interactions is done in a first-in, last-out approach. The first batch of
covariates will be tested for interactions with itself, then for with all covariates from
subsequent batches. The second batch does not need to test for interactions with the first one,
since the first one already did that. This allows the second batch to be done before the first one,
hence the "first-in, last-out" naming. The number of batches will always be a multiple of
two times the number of worker cores; this should ensure that all workers should be done
at the same time.

There is quite some code-duplication going on (DRY! OMG! U NUB), but that's for a reason. Function
calls in R have a tiny bit of overhead, and since we would do these function-calls a lot (and I mean a LOT)
of times, I have opted to simply copy the function's code into this function itself, and forego
the function itself.

Since this function is not exported, I will discusse the 6 indices found in the for-loops
(at some point we are 4 for-loops deep)
first.batch.real.indices
first/second.local.index
first/second.covariate.index

passed.indices are not used here (only returned), since amount.rejections is better since
possible NAs have been removed.

Reading from given files in combination with a maximum batchsize becomes... interesting.
Firstly: note that opening a connection to a file has overhead, so we wish to
avoid doing this as much as possible. This leads us to the following consideration.
Since the files, and consequently the covariates within, are fixed and determined before
the results of the first stage are available, we can have... suboptimal... cases. What if
all covariates from one file are all rejected? Or only one for each file? What do we do then?
Ideally, we would put all the rejected covariates in new files, matching the sizes of the
original files (to allow for memory-usage control), but that would be non-sensical. That is,
because then we would read all files individually into memory, and then write to storage again,
and finally read into memory again, and only then, after all this reading and writing would
the analysis begin.
The solution I chose is to group subsequent files together, in such a way that the relevant,
i.e. rejected in first stage, covariates from one group do not exceed the amount of covariates
typically found in one file. That way the memory-usage can still be controlled by the user,
by adjusting their files (which they would have done already for the first stage), while still
having a reasonable performance. It is true that this leads to inefficiencies. For example, say
that the first 3 batches have 40\% of their covariates rejected in the first stage. Then in the
second stage, we will only read the first 2 batches, resulting in 20\% of the memory-capacity not
being used optimally, but this is a worthwile sacrifice in my opinion.
Another option would be to write or find a optimization algorithm that would rearrange the batches
so the abovementioned method would waste as little memory as possible. But that algorithm would
also introduce overhead, and e.g. a naive approach to this problem (try every possible permutation)
would be of complexity O(n!) with n the number of batches. Maybe an efficient algorithm exists,
but I'm too lazy to implement this. The number of batches should ideally be too large, so it is
not that big of a deal. Anyway, that is therefore why the indices for each batch may look odd.
}
